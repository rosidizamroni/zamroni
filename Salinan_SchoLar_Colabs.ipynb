{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install scholarly --upgrade"
      ],
      "metadata": {
        "id": "6gemyh5_Q_qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "id": "RLsSgnvRUFHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scholarly import scholarly\n",
        "import pandas as pd\n",
        "\n",
        "# Cari profil berdasarkan nama\n",
        "search_query = scholarly.search_author('rosidi zamroni unisla')\n",
        "\n",
        "# Mengambil hasil pertama dari pencarian\n",
        "author = next(search_query)\n",
        "\n",
        "# Menampilkan informasi dasar\n",
        "print(f\"Name: {author['name']}\")\n",
        "print(f\"Affiliation: {author['affiliation']}\")\n",
        "print(f\"Interests: {author.get('interests', 'N/A')}\")\n",
        "print(f\"H-index: {author.get('hindex', 'N/A')}\")\n",
        "print(f\"I10-index: {author.get('i10index', 'N/A')}\")\n",
        "\n",
        "# List untuk menyimpan hasil publikasi\n",
        "data = []\n",
        "\n",
        "# Mengambil publikasi dari profil\n",
        "publications = scholarly.fill(author, sections=['publications'])['publications']\n",
        "for pub in publications:\n",
        "    pub = scholarly.fill(pub)\n",
        "\n",
        "    title = pub['bib']['title']\n",
        "    authors = pub['bib']['author']\n",
        "    journal = pub['bib'].get('journal', 'N/A')\n",
        "    year = pub['bib'].get('pub_year', pub['bib'].get('year', 'N/A'))\n",
        "    citations = pub.get('num_citations', 'N/A')\n",
        "    link = pub.get('pub_url', pub.get('url', pub.get('eprint', pub.get('url_scholarbib', 'N/A'))))\n",
        "\n",
        "    # Menyimpan data ke dalam list\n",
        "    data.append({\n",
        "        \"Title\": title,\n",
        "        \"Authors\": authors,\n",
        "        \"Journal\": journal,\n",
        "        \"Year\": year,\n",
        "        \"Citations\": citations,\n",
        "        \"Link\": link\n",
        "    })\n",
        "\n",
        "# Membuat DataFrame dari list\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Menyimpan DataFrame ke file Excel\n",
        "df.to_excel(\"zamroni unisla.xlsx\", index=False)\n",
        "\n",
        "print(\"Data berhasil disimpan ke google_scholar_publications.xlsx\")"
      ],
      "metadata": {
        "id": "PyN7sOPTUGIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scholarly import scholarly\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def fetch_publications(author_name):\n",
        "    try:\n",
        "        # Cari profil berdasarkan nama\n",
        "        search_query = scholarly.search_author(author_name)\n",
        "        author = next(search_query)\n",
        "\n",
        "        # Mengambil publikasi dari profil\n",
        "        publications = scholarly.fill(author, sections=['publications'])['publications']\n",
        "\n",
        "        # List untuk menyimpan hasil publikasi\n",
        "        data = []\n",
        "        for pub in publications:\n",
        "            pub = scholarly.fill(pub)\n",
        "            title = pub['bib']['title']\n",
        "            authors = pub['bib']['author']\n",
        "            journal = pub['bib'].get('journal', 'N/A')\n",
        "            year = pub['bib'].get('pub_year', pub['bib'].get('year', 'N/A'))\n",
        "            citations = pub.get('num_citations', 'N/A')\n",
        "            link = pub.get('pub_url', pub.get('url', pub.get('eprint', pub.get('url_scholarbib', 'N/A'))))\n",
        "\n",
        "            data.append({\n",
        "                \"Title\": title,\n",
        "                \"Authors\": authors,\n",
        "                \"Journal\": journal,\n",
        "                \"Year\": year,\n",
        "                \"Citations\": citations,\n",
        "                \"Link\": link\n",
        "            })\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {author_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "def main():\n",
        "    # Daftar nama akun Google Scholar yang ingin diproses\n",
        "    author_names = ['MARSHA SAVIRA AGATHA PUTRI unisla.ac.id','DENAYA ANDRYA PRASIDYA unisla.ac.id','MUHAMMAD HANIF unisla.ac.id','SUMIYATI unisla.ac.id','NUR LATHIFAH SYAKBANAH unisla.ac.id','MIMATUN NASIHAH unisla.ac.id','GADING WILDA ANIRIANI unisla.ac.id','EKO SULISTIONO unisla.ac.id','RIZKY RAHADIAN WICAKSONO, S.KM, M.KKK unisla.ac.id']\n",
        "\n",
        "    # Menyimpan semua hasil ke dalam satu list besar\n",
        "    all_data = []\n",
        "\n",
        "    # Menggunakan ThreadPoolExecutor untuk menjalankan tugas secara paralel\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # Membuat futures untuk setiap akun penulis\n",
        "        futures = [executor.submit(fetch_publications, name) for name in author_names]\n",
        "\n",
        "        # Menyimpan hasil ketika tugas selesai\n",
        "        for future in as_completed(futures):\n",
        "            all_data.extend(future.result())\n",
        "\n",
        "    # Membuat DataFrame dari hasil yang terkumpul\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Menyimpan DataFrame ke file Excel\n",
        "    df.to_excel(\"google_scholar_multiple_accounts1.xlsx\", index=False)\n",
        "    print(\"Data berhasil disimpan ke google_scholar_multiple_accounts_paralel.xlsx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "06Lrb7spUPcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qqq"
      ],
      "metadata": {
        "id": "p7nzfPetz872"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scholarly import scholarly\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def fetch_publications(author_name):\n",
        "    try:\n",
        "        # Cari profil berdasarkan nama\n",
        "        search_query = scholarly.search_author(author_name)\n",
        "        author = next(search_query)\n",
        "\n",
        "        # Mengambil publikasi dari profil\n",
        "        publications = scholarly.fill(author, sections=['publications'])['publications']\n",
        "\n",
        "        # List untuk menyimpan hasil publikasi\n",
        "        data = []\n",
        "        for pub in publications:\n",
        "            pub = scholarly.fill(pub)\n",
        "            title = pub['bib']['title']\n",
        "            authors = pub['bib']['author']\n",
        "\n",
        "            # Memeriksa apakah publikasi adalah jurnal atau konferensi/prosiding\n",
        "            journal = pub['bib'].get('journal')  # Jurnal\n",
        "            conference = pub['bib'].get('conference')  # Konferensi atau Prosiding\n",
        "\n",
        "            if journal:\n",
        "                publication_type = 'Journal'\n",
        "                venue = journal\n",
        "            elif conference:\n",
        "                publication_type = 'Conference/Proceeding'\n",
        "                venue = conference\n",
        "            else:\n",
        "                publication_type = 'Other'\n",
        "                venue = 'N/A'\n",
        "\n",
        "            year = pub['bib'].get('pub_year', pub['bib'].get('year', 'N/A'))\n",
        "            citations = pub.get('num_citations', 'N/A')\n",
        "            link = pub.get('pub_url', pub.get('url', pub.get('eprint', pub.get('url_scholarbib', 'N/A'))))\n",
        "\n",
        "            data.append({\n",
        "                \"Title\": title,\n",
        "                \"Authors\": authors,\n",
        "                \"Type\": publication_type,\n",
        "                \"Venue\": venue,\n",
        "                \"Year\": year,\n",
        "                \"Citations\": citations,\n",
        "                \"Link\": link\n",
        "            })\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {author_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "def main():\n",
        "    # Daftar nama akun Google Scholar yang ingin diproses\n",
        "    author_names = ['azizah affandy universitas islam lamongan']\n",
        "\n",
        "    # Menyimpan semua hasil ke dalam satu list besar\n",
        "    all_data = []\n",
        "\n",
        "    # Menggunakan ThreadPoolExecutor untuk menjalankan tugas secara paralel\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # Membuat futures untuk setiap akun penulis\n",
        "        futures = [executor.submit(fetch_publications, name) for name in author_names]\n",
        "\n",
        "        # Menyimpan hasil ketika tugas selesai\n",
        "        for future in as_completed(futures):\n",
        "            all_data.extend(future.result())\n",
        "\n",
        "    # Membuat DataFrame dari hasil yang terkumpul\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Menyimpan DataFrame ke file Excel\n",
        "    df.to_excel(\"sipil2.xlsx\", index=False)\n",
        "    print(\"Data berhasil disimpan ke google_scholar_multiple_accounts1.xlsx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XjTNsC_B3EPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scholarly import scholarly\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def fetch_publications_by_affiliation(affiliation_keywords):\n",
        "    try:\n",
        "        data = []\n",
        "\n",
        "        for keyword in affiliation_keywords:\n",
        "            # Cari profil penulis berdasarkan kata kunci\n",
        "            search_query = scholarly.search_author(keyword)\n",
        "\n",
        "            for author in search_query:\n",
        "                author = scholarly.fill(author, sections=['publications'])\n",
        "\n",
        "                # Loop melalui profil yang ditemukan, periksa apakah afiliasinya mengandung salah satu keyword\n",
        "                if any(kw.lower() in author['affiliation'].lower() for kw in affiliation_keywords):\n",
        "                    publications = author['publications']\n",
        "\n",
        "                    # Mengambil publikasi dari profil\n",
        "                    for pub in publications:\n",
        "                        pub = scholarly.fill(pub)\n",
        "                        title = pub['bib']['title']\n",
        "                        all_authors = pub['bib']['author']\n",
        "                        primary_author = author['name']\n",
        "\n",
        "                        # Memeriksa apakah publikasi adalah jurnal atau konferensi/prosiding\n",
        "                        journal = pub['bib'].get('journal')  # Jurnal\n",
        "                        conference = pub['bib'].get('conference')  # Konferensi atau Prosiding\n",
        "\n",
        "                        if journal:\n",
        "                            publication_type = 'Journal'\n",
        "                            venue = journal\n",
        "                        elif conference:\n",
        "                            publication_type = 'Conference/Proceeding'\n",
        "                            venue = conference\n",
        "                        else:\n",
        "                            publication_type = 'Other'\n",
        "                            venue = 'N/A'\n",
        "\n",
        "                        year = pub['bib'].get('pub_year', pub['bib'].get('year', 'N/A'))\n",
        "                        citations = pub.get('num_citations', 'N/A')\n",
        "                        link = pub.get('pub_url', pub.get('url', pub.get('eprint', pub.get('url_scholarbib', 'N/A'))))\n",
        "\n",
        "                        # Menampilkan detail artikel yang sedang diambil\n",
        "                        print(f\"\\nFetching publication:\\nTitle: {title}\\nPrimary Author: {primary_author}\\nLink: {link}\")\n",
        "\n",
        "                        data.append({\n",
        "                            \"Title\": title,\n",
        "                            \"Primary Author\": primary_author,\n",
        "                            \"All Authors\": all_authors,\n",
        "                            \"Type\": publication_type,\n",
        "                            \"Venue\": venue,\n",
        "                            \"Year\": year,\n",
        "                            \"Citations\": citations,\n",
        "                            \"Link\": link\n",
        "                        })\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for keywords {affiliation_keywords}: {e}\")\n",
        "        return []\n",
        "\n",
        "def main():\n",
        "    # Daftar kata kunci afiliasi yang ingin diproses\n",
        "    affiliation_keywords = ['unisla', 'unisla.ac.id', 'universitas islam lamongan']\n",
        "\n",
        "    # Menyimpan semua hasil ke dalam satu list besar\n",
        "    all_data = []\n",
        "\n",
        "    # Menggunakan ThreadPoolExecutor untuk menjalankan tugas secara paralel\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # Membuat futures untuk setiap set kata kunci afiliasi\n",
        "        futures = [executor.submit(fetch_publications_by_affiliation, [keyword]) for keyword in affiliation_keywords]\n",
        "\n",
        "        # Menyimpan hasil ketika tugas selesai\n",
        "        for future in as_completed(futures):\n",
        "            all_data.extend(future.result())\n",
        "\n",
        "    # Membuat DataFrame dari hasil yang terkumpul\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Menyimpan DataFrame ke file Excel\n",
        "    df.to_excel(\"google_scholar_affiliations_keywords.xlsx\", index=False)\n",
        "    print(\"Data berhasil disimpan ke google_scholar_affiliations_keywords.xlsx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "CxQMYyEorSQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxbD86WoB5gl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}